ANCOVA
> df <- read.csv('vehicles.csv')
> any(is.na(df))
> na.omit(df) # or df[complete.cases(df),]
> pl1 <-ggplot(df,aes(type,price ))+geom_boxplot()
> df <- df[df$type == "Automobile"& df$price < 60,]
#Now we have a data without significant outliers and we can check for a homogeneity of regression slopes or level of independance
#between our factor and covariate.

> model <- aov(sales~type*price, df)
> av <- Anova(model,type='III')
> av
Anova Table (Type III tests)

Response: sales
            Sum Sq  Df F value    Pr(>F)    
(Intercept) 144184   1 33.4606 4.554e-08 ***
type          3175   1  0.7368  0.392139    
price        45953   1 10.6642  0.001373 ** 
type:price     179   1  0.0415  0.838872    
Residuals   603268 140  

#Since p-value is bigger than 5 we can conclude that there is no sighnificant relationship between factor and covariate, so the assumption
#of absence of homogenity is met. In case the relationship is statistically significant we cannot run the test and need to seek for a help
#of non-parametric tests as Johnson-Neyman Procedure. 
> res <- residuals(model)
> zres <- scale(res)
> shapiro.test(zres)

	Shapiro-Wilk normality test

data:  zres
W = 0.72672, p-value = 4.712e-15
#Since the p-value is lower than 5% we can conclude that data is not normally distributed and we reject
# a null hypothesis (the data is normally distributed)and we accept alternative hypothesis. So this assumption is not met. 
#Let`s move on to checking the homogeneity if variances using Levene Test from packege car. 
> leveneTest(df$sales,df$type)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value  Pr(>F)  
group   1  5.0065 0.02681 *
      142
#Since the p-value is lower than 5% we can say there is no homogeneity of variances. The assumption is not met. 

> pred <- predict(model)
> ggplot(df)+geom_point(aes(x=pred,y=zres))
#On the scatter plot we can see that all points are gatgered in one place, so we can conclde that there is no homoscedasticity so
# this assumption is not met. The scatterplot shows us a cone-pattern shape that is quite typical for heteroscedasticity. 

#After testing assumptions let`s get back to our ANCOVA.
> model2 <- aov(sales~type+price,df)
> ancova <- Anova(model2,type='III')
> ancova
Anova Table (Type III tests)

Response: sales
            Sum Sq  Df F value    Pr(>F)    
(Intercept) 169462   1 39.5960 3.697e-09 ***
type         42659   1  9.9677 0.0019487 ** 
price        54547   1 12.7453 0.0004885 ***
Residuals   603447 141                      
#Now we can say that the influence of type of car on sales is significant and it is adjusted by the influence of covariate(price).

> library(effects)
> effect('type',model2)
 type effect
type
Automobile      Truck 
  45.57924   84.33722 
> library(multcomp)
> mcomp <- glht(model2, linfct = mcp(type ='Tukey'))
> summary(mcomp)

	 Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: aov(formula = sales ~ type + price, data = df)

Linear Hypotheses:
                        Estimate Std. Error t value Pr(>|t|)   
Truck - Automobile == 0    38.76      12.28   3.157  0.00195 **
#Now we can say that the p-value is less than 5% so the difference in price between Truck and Automile is statistically significant
#And the sales for trucks are much higher than for Automobiles. The confidece intervals are computed below. 
> confint(mcomp)

Multiple Comparisons of Means: Tukey Contrasts
                        Estimate lwr     upr    
Truck - Automobile == 0 38.7580  14.4888 63.0271
//////////////////////////////////////////////////////
WITHIN-SUBJECTS ANOVA ( REPEATED MEASURES)
#Moving to the next df. This df contains measurments of people`s weight during their diet for five times. We will figure out
# weather the diet is effective or not and which stage helps to get rid of the bigger amout of weight. 
#First of all we need to check our assumptions. Shapiro-Wilks test is conducted below and all p-values are more than 5%. Data is normal.

df.shapiro <- apply(df,2,shapiro.test)
> df.shapiro

$weight0
	Shapiro-Wilk normality test
data:  newX[, i]
W = 0.93756, p-value = 0.3201
$weight1
	Shapiro-Wilk normality test
data:  newX[, i]
W = 0.94985, p-value = 0.4874
$weight2
	Shapiro-Wilk normality test
data:  newX[, i]
W = 0.95354, p-value = 0.5478
$weight3
	Shapiro-Wilk normality test
data:  newX[, i]
W = 0.94113, p-value = 0.3631
$weight4
	Shapiro-Wilk normality test
data:  newX[, i]
W = 0.94939, p-value = 0.480

> library(reshape2)
> df1 <- melt(df1)
> ggplot(df1,aes(x=variable,y=value))+geom_boxplot()
#No outliers at all, our data is good to go. Btw we can see the slight decreas in median weight on the boxplot. Right now it seems that
#drop of weight from period 1 to period 2 is the biggest.

> moments_mat <- c('period1','period2','period3','period4','period5')
> moments_df <- as.data.frame(moments_mat)
> mat1 <- as.matrix(,df[2:6])
Error in as.matrix.default(, df[2:6]) : 
  argument "x" is missing, with no default
> mat1 <- as.matrix(df[,2:6])
> View(mat1)
> model <- lm(mat1~1)
> summary(model)
Response weight0 :

Call:
lm(formula = weight0 ~ 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-47.375 -29.875   0.125  26.375  58.625 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  198.375      8.368   23.71 2.65e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.47 on 15 degrees of freedom


Response weight1 :

Call:
lm(formula = weight1 ~ 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-50.125 -29.375   1.875  25.375  58.875 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  196.125      8.417    23.3 3.41e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.67 on 15 degrees of freedom


Response weight2 :

Call:
lm(formula = weight2 ~ 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-50.125 -28.375   0.375  24.375  59.875 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  194.125      8.375   23.18 3.68e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.5 on 15 degrees of freedom


Response weight3 :

Call:
lm(formula = weight3 ~ 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-48.125 -30.125  -0.125  26.125  59.875 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  192.125      8.479   22.66 5.13e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.92 on 15 degrees of freedom


Response weight4 :

Call:
lm(formula = weight4 ~ 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-50.312 -29.312   2.188  26.438  58.688 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  190.312      8.377   22.72 4.94e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.51 on 15 degrees of freedom

> model2 <- Anova(model,idata=moments_df,idesign=~moments_mat,type="III")
> summary(model2, multivariate=F)
Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

             Sum Sq num Df Error SS den Df F value    Pr(>F)    
(Intercept) 3017480      1    84579     15 535.150 3.793e-13 ***
moments_mat     649      4      158     60  61.668 < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Mauchly Tests for Sphericity

            Test statistic p-value
moments_mat        0.43827 0.27459
Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity
             GG eps Pr(>F[GG])    
moments_mat 0.77425  < 2.2e-16 ***
---
               HF eps   Pr(>F[HF])
moments_mat 0.9987452 1.471091e-20

#Since Mauchly test for sphericity has a p-value >5% this assumption is met, so we look at the table above and conclude that the difference
#is significant.As a result you can recommend this diet to your friends, they will be happy with that :))
#We already have our reshaped data, so now we can see in quantitative differences between various time of the diet.
> model3 <- aov(value~variable,df1)
> TukeyHSD(model3)
  Tukey multiple comparisons of means
    95% family-wise confidence level
Fit: aov(formula = value ~ variable, data = df1)
$`variable`
                   diff       lwr      upr     p adj
weight1-weight0 -2.2500 -35.46851 30.96851 0.9997063
weight2-weight0 -4.2500 -37.46851 28.96851 0.9964257
weight3-weight0 -6.2500 -39.46851 26.96851 0.9844406
weight4-weight0 -8.0625 -41.28101 25.15601 0.9605136
weight2-weight1 -2.0000 -35.21851 31.21851 0.9998160
weight3-weight1 -4.0000 -37.21851 29.21851 0.9971754
weight4-weight1 -5.8125 -39.03101 27.40601 0.9881501
weight3-weight2 -2.0000 -35.21851 31.21851 0.9998160
weight4-weight2 -3.8125 -37.03101 29.40601 0.9976571
weight4-weight3 -1.8125 -35.03101 31.40601 0.9998756
#It seems to have p-value close to 1, I assume this is so as we have only 16 observations in each group and this is not enough to prove
#significance. Though you can take a loot at differences and check weahter it is acceptable for person or not.If you remembered, I tried 
#to guess the most effective period for loosing weight judging the boxplot, and I noticed the most significant drop in perions 2-1,
#in fact we can see that the biggest drop(if we take one period i.e. 4-3) was mentioned in the begginning 1-0 with -2.25 lost lb.
#Than we can see that the lost weight was accounted for 2 lb in following periods and in the end it was -1.81 lb. So the most productive
#period for loosing weight is the beginning of the diet.

////////////////////////////////////////////////////////
MIXED ANOVA
#Let`s move to another analysis. The data set is the same, but we added there gender so now we will perform MIXED ANOVA. 
#1.Checking the assumption homogeneity of variances:
> leveneTest(df$weight0,df$gender)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  1  1.3719  0.261
      14               
> leveneTest(df$weight1,df$gender)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  1   0.208 0.6554
      14               
> leveneTest(df$weight2,df$gender)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  1  0.7135 0.4125
      14               
> leveneTest(df$weight3,df$gender)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  1  0.9996 0.3344
      14               
> leveneTest(df$weight4,df$gender)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  1  0.6883 0.4206
      14               
#All five tests have p-value >5% so the null hypothesis is accepted. Variances are equal.
#2.Checking the assumotion if homogeneity of covariances.
> df2 <- df[,2:6]
> boxM(df2, df$gender)
	Box's M-test for Homogeneity of Covariance Matrices
data:  df2
Chi-Sq (approx.) = 20.714, df = 15, p-value = 0.1462
#P-value is > 5% so there is homogeneity of covariances.Assumption is met so we move further.
#We have finished with checking assuptions, let`s move on to the test. Tha assumption of sphericity will be tested during during ANOVA test.
#Let`s prepare data for analysis. First we need to have a df with all possible combinations of our factors(2x5). 
> bla1  <- c('weight0','weight1','weight2','weight3','weight4','weight0','weight1','weight2','weight3','weight4')
> bla2 <- c('male','male','male','male','male','female','female','female','female','female')
> df.comb <- data.frame(bla1,bla2)
#Now we are separating males and females in different df.Also we create matrixes with only weights. 
> weight.m <- df[df$gender==1,]
> weight.f <- df[df$gender==0,]
> colnames(weight.f)<- c('gender',"1_fem","2_fem","3_fem","4_fem","5_fem")
> colnames(weight.m)<- c('gender',"1_male","2_male","3_male","4_male","5_male")
> total <- cbind(weight.f$`1_fem`,weight.f$`2_fem`,weight.f$`3_fem`,weight.f$`4_fem`,weight.f$`5_fem`,weight.m$`1_male`,weight.m$`2_male`,weight.m$`3_male`,weight.m$`4_male`,weight.m$`5_male`)
model <- lm(total~1)
> model
Call:
lm(formula = total ~ 1)
Coefficients:
             [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]   [,10]
(Intercept)  223.8  221.4  219.2  217.7  215.7  167.6  165.8  163.9  161.2  159.7

#Since intersept is the mean for each column we can see averege weight here.Probably in the data set there was a mistake in interpretation.
#It was writen that 0-female and 1-male, but we can see that the weight in rows 6 to 10 is less than in 1 to 5 and as biologocally
#males tend to have more weight we can say that 1 to 5 are males ans 6 to 10 are females.

> colnames(df.comb) <- c('period','gender')
> model2 <- Anova(model,idata=df.comb,idesign = ~period + gender*period,type="III")
> summary(model2,multivariate = F)
Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

               Sum Sq num Df Error SS den Df   F value    Pr(>F)    
(Intercept)   3303567      1   8403.3      8 3145.0231 1.134e-11 ***
period            733      4     67.4     32   86.9868 < 2.2e-16 ***
gender          70392      1  11099.2      8   50.7367 9.971e-05 ***
period:gender       4      4     97.7     32    0.2875    0.8839    

Mauchly Tests for Sphericity
              Test statistic p-value
period               0.18005 0.29189
period:gender        0.19100 0.31897
Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

               GG eps Pr(>F[GG])    
period        0.55778  3.495e-10 ***
period:gender 0.62099     0.7978    

                 HF eps   Pr(>F[HF])
period        0.7835075 1.574571e-13
period:gender 0.9225660 8.706218e-01
#So here we firstly check for the spericity, it is >5% so this assumption is met. Than we are taking a look at interaction effect.
#The p-value for it is sustantionally bigger than 5%, so there is no intercation between them. No combined effect for gender and period,
#so we would not study main simple effect after. Though it seems to be insignificant due to the small number of observations.

////////////////////////////////////////////////////////////////////////////
#The last part here is a Friedman Test. Data set is the same.
> df.mat <- as.matrix(df2)
> friedman.test(df.mat)

	Friedman rank sum test

data:  df.mat
Friedman chi-squared = 53.411, df = 4, p-value = 6.991e-11
#Yes, this was easy, two lines of code and that`s it. The p-value is lower than 5% so the median defferences between different periods
#of the diet are important.
