#We are using data set marketing with the dependant variable Union. It is bonomial o we will perform binomial type of regression to predict
#whether they will be union member or not. There are three independant variables: age(continuous), satisfaction with the job(ordinal) and 
#political view(categorical nominal). 
#First of all we are checking assumptions of outliers and multicollinearity of independant variables. 

> df <- df[,c(2,6,7,8)]
> df.indep <- df[,c(1,2,4)]
> install.packages('usdm')
> vif(df.indep)
  Variables      VIF
1       age 1.216778
2    jobsat       NA
3   polview       NA
#The only numeric variable is age and the Variance Inflation Factor is less than 10, so we can continue as there is no multicollinearity. 
> ggplot(df,aes(union,age))+geom_boxplot()

#Now we can see that both groups do not present outliers at all, so we can continue with the regression.
> model <- glm(union~age+jobsat+polview,data=df,family=binomial)
> summary(model)
#The table quite large so I would not post it, though all parameters are not statistically significant as they are higher than 5%, though
#the Age has importance of 0.0521 and category of people  who has extremely conservative political view has p-value equal to 0.0865.
> expb <- exp(coef(model))
> expb
#Let`s take a look at coefficients which are close to be important and people with the higher age has 4% more chances to be a union
#member than younger people. Also, citizens with extremely conservative political views has 60.9% to be a union member. Other factors
#do not have importance. 

#Let`s continue with the goodness-of-fit calculations and we will start with Hosmer-Lemeshow statistics. It should be >5% if the model 
#performed well.
> library(ResourceSelection)
> hoslem.test(df$union,fitted(model))
	Hosmer and Lemeshow goodness of fit (GOF) test
data:  df$union, fitted(model)
X-squared = 5000, df = 8, p-value < 2.2e-16
> library(fmsb)
> NagelkerkeR2(model)
$`N`
[1] 5000
$R2
[1] 0.0038312
#As we can see, the model performed really poor, as it describes only 0.3% of results. Though let`s continue our exploration.
> library(BaylorEdPsych)
> PseudoR2(model)
        McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina 
    2.591051e-03    -3.574953e-03     2.182736e-03     3.831200e-03     4.964377e-03 
          Effron            Count        Adj.Count              AIC    Corrected.AIC 
    2.257911e-03               NA               NA     4.229744e+03     4.229806e+03 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

#Now we will be using the same data but we will choose different dependant and independant variables as we are going to perform
#multiniomial regression. Dpendant variable is the type of card:
> table (df$card)
American Express         Discover       Mastercard            Other             Visa 
            1000             1333             1202              215             1250 
#And our independant variables are: age(continuous), gender(dichotomus or binomial) and age(continuous).
#The assumptions remain the same as for the binomial regression: Outliers and multicollinearity. 

> df <- df[,c(1,2,3,5)]
> ggplot(df,aes(card,age))+geom_boxplot()
> ggplot(df,aes(card,income))+geom_boxplot()
#On the boxplot we can see numerous significant outliers that may negatively influence our model. To make our model cleaner, let`s
#get rid of the most important outliers, i.e. people with the salary higher than 300,000.
> df <- subset(df,income<300)
> ggplot(df,aes(card,income))+geom_boxplot()
#Looks a bit better right now, we have deleted 34 observations from 5,000 and now we can continue with the multinomial analysis.
#First of all we will pick up a reference category for each variable that is categorical, both dependant and independant.

> df$gender <- relevel(df$gender, ref = 'Male')
> df$card <- relevel(df$card, ref = "Other")
> library(nnet)
> model<- multinom(card~gender+age+income,data=df)
> summary(model)
Coefficients:
                 (Intercept) genderFemale          age     income
American Express    1.252334   -0.8324198 -0.002090151 0.01793280
Discover            1.054368   -0.4531345  0.002069272 0.01997840
Mastercard          1.141782   -0.3550897  0.005953687 0.01224755
Visa                1.281010   -0.2452058  0.004374856 0.01042260

#We now have coefficients of the model, though it is better to interpret their antilogariphms.
> summ <- summary(model)
> z <- summ$coefficients/summ$standard.errors
> pv <- pnorm(abs(z),lower.tail = F)*2
> pv
                  (Intercept) genderFemale       age       income
American Express 8.323888e-08 7.884441e-08 0.6127259 5.787055e-10
Discover         4.747736e-06 2.804839e-03 0.6065860 3.063833e-12
Mastercard       7.508746e-07 1.941748e-02 0.1351036 2.324264e-05
Visa             2.452345e-08 1.054975e-01 0.2695227 3.293312e-04
#As we can see from the model results, all categories and variables except Age are statistically significant and they differ from
#our baselines. Also we can change our baselines to see the picture from the other side.

#
> expb <- exp(coef(model))
> expb
                 (Intercept) genderFemale      age   income
American Express    3.498497    0.4349954 0.997912 1.018095
Discover            2.870162    0.6356326 1.002071 1.020179
Mastercard          3.132345    0.7011106 1.005971 1.012323
Visa                3.600272    0.7825435 1.004384 1.010477
#Judjing from this table we can see  that most commonly the person would prefere on of AE-Visa cards comparing to other. The same is with
#the income. THe richer person is the more chances for him to choose one of the most popular financial companies from the list
#comparing to our baseline category Other. Also, it is more common that females will coose Other category expecially females would not
#choose American Express and Discover whilst these two financial companies are usually chosen by people with the higher income.

> ci <-confint(model,level=0.95)    #creating confidential intervals for each category(only one is displayed) 
> ci
, , American Express

                   2.5 %       97.5 %
(Intercept)   0.79439595  1.710271122
genderFemale -1.13625360 -0.528585926
age          -0.01018326  0.006002958
income        0.01226028  0.023605325
> exci <- exp(ci)               #ci for antilogs
> pred <- fitted(model)
> pred
            Other American Express  Discover Mastercard       Visa
1    0.0669294794        0.1703180 0.2364151 0.24203430 0.28430314
2    0.0544911434        0.2382643 0.2208771 0.23381131 0.25255615

#Giving you these two rows just as refence and a proof that our model performed really poorly. The probablilities for each category
#varies around +-25%  and obviously a small probability that person will choose Other category. We may reach almost the same effect by 
#random assigning of category. The model performed poorly, though we did all of this just for the studying purposes.

#Performimg goodness of fit 
> library(nnet)
> model<- multinom(card~gender+age+income,data=df)
> model0<- multinom(card~1,data = df)
> LL1 <- logLik(model)
> LL0 <- logLik(model0)
> mccfadden <- 1 - (LL1/LL0)
> mccfadden
'log Lik.' 0.0226665 (df=16)
> n <- nrow(df)
> nagel <- (1 -exp((2/n)*(LL0-LL1)))/(1 - exp(LL0)^(2/n))
> coxsnell <- (1 -exp((2/n)*(LL0-LL1)))
> nagel
'log Lik.' 0.06569196 (df=4)
> coxsnell
'log Lik.' 0.06569196 (df=4)
# 6% of goodness of fit proves my statement above regarding the poor accuracy of the model. We have manually computed 3 different
#types of r using loglikelyhoods of fitted and null model. So we compared how much our model is better than the model without any
#dependant variables. 

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
#Ordinal Regression
#So here we continue our work with the same dataframe though now our dependent variable is jobsat (job satisfaction) and the 
#independent variables are age, gender, income and ed (education level).

> df <- read.csv('marketingdb.csv')
> df <- df[,c(6,2,1,3,4)]
> head(df)
                 jobsat age gender income ed
1   Highly dissatisfied  20 Female     31 15
2   Highly dissatisfied  22   Male     15 17
3    Somewhat satisfied  67 Female     35 14
4 Somewhat dissatisfied  23   Male     20 16

#So we will determine how accurate our four independant variables define our josat.Firstly we will do some EDA and we will perform 
#checking of assumptions. 

> table(df$jobsat)
  Highly dissatisfied      Highly satisfied Neutral Somewhat dissatisfied   Somewhat satisfied 
                967                   886          1092                  1041   		1014


> df$jobsat <- relevel(factor(df$jobsat), ref = 'Neutral')
> df$gender <- relevel(factor(df$gender), ref = 'Male')
> model <- polr(factor(jobsat)~age+gender+income+ed, data = df, method = 'logistic', Hess = T)
> summary(model)
Call:
polr(formula = factor(jobsat) ~ age + gender + income + ed, data = df, 
    Hess = T, method = "logistic")

Coefficients:
                  Value Std. Error t value
age           0.0113892   0.001445  7.8811
genderFemale -0.0637020   0.050025 -1.2734
income        0.0009975   0.000481  2.0739
ed           -0.0025011   0.007811 -0.3202

Intercepts:
                                         Value   Std. Error t value
Neutral|Highly dissatisfied              -0.7849  0.1400    -5.6074
Highly dissatisfied|Highly satisfied      0.1509  0.1395     1.0816
Highly satisfied|Somewhat dissatisfied    0.8822  0.1403     6.2872
Somewhat dissatisfied|Somewhat satisfied  1.8973  0.1424    13.3266

Residual Deviance: 15988.12 
AIC: 16004.12

#So now we need to interpret a model and define which variables are statistically impotant. For this we will use Anova function from
#pakage 'car'. As we can see below there are two variables that influence the class of Satisfaction with the job and thay are: age and 
#income. Suprisingly education is not a big deal for being satisfied with your job. 
> Anova(model)
Analysis of Deviance Table (Type II tests)

Response: factor(jobsat)
       LR Chisq Df Pr(>Chisq)    
age      62.371  1  2.845e-15 ***
gender    1.622  1    0.20286    
income    4.349  1    0.03704 *  
ed        0.103  1    0.74878    

#We can do this in a different way to get detailed p-values for each factor: 
> df.coef <- coef(summary(model))
> p <- pnorm(abs(df.coef[,'t value']), lower.tail = F)*2
> p
                                     age                             genderFemale 
                            3.243848e-15                             2.028770e-01 
                                  income                                       ed 
                            3.809227e-02                             7.488099e-01 
             Neutral|Highly dissatisfied     Highly dissatisfied|Highly satisfied 
                            2.054139e-08                             2.794414e-01 
  Highly satisfied|Somewhat dissatisfied Somewhat dissatisfied|Somewhat satisfied 
                            3.231713e-10                             1.621019e-40  
> table.coef <- cbind(df.coef, 'p value'=p)
> table.coef
> table.coef
                                                 Value   Std. Error   t value      p value
age                                       0.0113892425 0.0014451247  7.881149 3.243848e-15
genderFemale                             -0.0637019706 0.0500252024 -1.273398 2.028770e-01
income                                    0.0009974852 0.0004809799  2.073861 3.809227e-02
ed                                       -0.0025011380 0.0078109542 -0.320209 7.488099e-01
Neutral|Highly dissatisfied              -0.7848834311 0.1399733238 -5.607379 2.054139e-08
Highly dissatisfied|Highly satisfied      0.1508745814 0.1394952610  1.081575 2.794414e-01
Highly satisfied|Somewhat dissatisfied    0.8821883048 0.1403142064  6.287234 3.231713e-10
Somewhat dissatisfied|Somewhat satisfied  1.8972564990 0.1423660464 13.326608 1.621019e-40

> ci <- confint(model)
> ci
                     2.5 %      97.5 %
age           8.559821e-03 0.014222750
genderFemale -1.617666e-01 0.034343417
income        6.006846e-05 0.001937978
ed           -1.780932e-02 0.012806877
#Usually if the CI does not contain 0 it means that our variable is important and does contribute to the defining of dependant variable.
#In our case gender and educaction are not statistically significant, though Income and Age are crusial (as Anova showed).

#Now we can continue to calculation our odds ratio, as these coeficient are easier to interpret. 

> odds.ratio <- exp(coef(model))
> odds.ratio
      age genderFemale       income           ed 
   1.0114543    0.9382846    1.0009980    0.9975020 
#Finally we can interpret the cofficients. The first variable age is a continuous variable so we will compare high and low values. The older
#person has 1,1% more chanses to be satisfied then his young colleague. Second varibale is dichotomous and Females are compared to males
# As males is our reference category. So Female workers have 6% less chanses to be satisfied then males. Income and education variables 
#are very close to 1, so there is almost no difference between people who earn a bit and a lot and people with a lot of diplomas or just
#a high school diploma. 

> ci.odds <- exp(confint(model))
> ci.odds
              2.5 %   97.5 %
age          1.0085966 1.014324
genderFemale 0.8506397 1.034940
income       1.0000601 1.001940
ed           0.9823483 1.012889
#So right now we can interpret our odds. We will be doing this for the income and age as they are the only variables that have p-value <5%.
# The higher is income the higher chance for our observations to be more satisfied. 
# In fact we can see that the difference is not so crucial, but we remember that variance of income:
> summary(df$income)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   9.00   24.00   38.00   52.45   66.00  296.00 

#To calculate goodness of fit we will come up with three coeficients called pseudoR
>  model0 <- polr(factor(jobsat)~1,data = df, method = 'logistic', Hess = T)
>  LL0 <- logLik(model0)
>  ll1 <- logLik(model)
>  LL1 <- logLik(model)
>  mcfadden <- 1 - LL1/LL0
>  mcfadden <- 1 - (LL1/LL0)
>  n <- nrow(df)
>  coxsnell <- 1 - exp((2/n)*(LL0-LL1))
>  nagel <- (1 - exp((2/n)*(LL0-LL1)))/(1-exp(LL0)^(2/n))
>  pseudo.r <- cbind('McFadden'=mcfadden, 'Cox-Snell'=coxsnell, 'Nagelkerke' = nagel)
>  pseudo.r
        McFadden  Cox-Snell Nagelkerke
[1,] 0.005075935 0.01618139 0.01618139

#Now we can check for the proportional odds assumption. 
> library(ordinal)
> model1 <- clm(factor(jobsat)~gender+age+income+ed, data= df)
> nominal_test(model1)
Tests of nominal effects

formula: factor(jobsat) ~ gender + age + income + ed
       Df  logLik   AIC     LRT  Pr(>Chi)    
<none>    -7994.1 16004                      
gender  3 -7992.8 16008   2.584    0.4603    
age                                          
income  3 -7869.0 15760 250.021 < 2.2e-16 ***
ed      3 -7977.9 15978  32.406 4.297e-07 ***

#As we can see from this model, the assumption of proportional odds is met ony for gender, while Income an education does not have proportional
#odds. This tells us we cannot use polr model to study our data and other options to do this is either partial proportional odds model or 
#using a graphical method to determine proportional odds to double check it. 






