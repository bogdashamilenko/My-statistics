#We are using data set marketing with the dependant variable Union. It is bonomial o we will perform binomial type of regression to predict
#whether they will be union member or not. There are three independant variables: age(continuous), satisfaction with the job(ordinal) and 
#political view(categorical nominal). 
#First of all we are checking assumptions of outliers and multicollinearity of independant variables. 

> df <- df[,c(2,6,7,8)]
> df.indep <- df[,c(1,2,4)]
> install.packages('usdm')
> vif(df.indep)
  Variables      VIF
1       age 1.216778
2    jobsat       NA
3   polview       NA
#The only numeric variable is age and the Variance Inflation Factor is less than 10, so we can continue as there is no multicollinearity. 
> ggplot(df,aes(union,age))+geom_boxplot()

#Now we can see that both groups do not present outliers at all, so we can continue with the regression.
> model <- glm(union~age+jobsat+polview,data=df,family=binomial)
> summary(model)
#The table quite large so I would not post it, though all parameters are not statistically significant as they are higher than 5%, though
#the Age has importance of 0.0521 and category of people  who has extremely conservative political view has p-value equal to 0.0865.
> expb <- exp(coef(model))
> expb
#Let`s take a look at coefficients which are close to be important and people with the higher age has 4% more chances to be a union
#member than younger people. Also, citizens with extremely conservative political views has 60.9% to be a union member. Other factors
#do not have importance. 

#Let`s continue with the goodness-of-fit calculations and we will start with Hosmer-Lemeshow statistics. It should be >5% if the model 
#performed well.
> library(ResourceSelection)
> hoslem.test(df$union,fitted(model))
	Hosmer and Lemeshow goodness of fit (GOF) test
data:  df$union, fitted(model)
X-squared = 5000, df = 8, p-value < 2.2e-16
> library(fmsb)
> NagelkerkeR2(model)
$`N`
[1] 5000
$R2
[1] 0.0038312
#As we can see, the model performed really poor, as it describes only 0.3% of results. Though let`s continue our exploration.
> library(BaylorEdPsych)
> PseudoR2(model)
        McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina 
    2.591051e-03    -3.574953e-03     2.182736e-03     3.831200e-03     4.964377e-03 
          Effron            Count        Adj.Count              AIC    Corrected.AIC 
    2.257911e-03               NA               NA     4.229744e+03     4.229806e+03 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

#Now we will be using the same data but we will choose different dependant and independant variables as we are going to perform
#multiniomial regression. Dpendant variable is the type of card:
> table (df$card)
American Express         Discover       Mastercard            Other             Visa 
            1000             1333             1202              215             1250 
#And our independant variables are: age(continuous), gender(dichotomus or binomial) and age(continuous).
#The assumptions remain the same as for the binomial regression: Outliers and multicollinearity. 

> df <- df[,c(1,2,3,5)]
> ggplot(df,aes(card,age))+geom_boxplot()
> ggplot(df,aes(card,income))+geom_boxplot()
#On the boxplot we can see numerous significant outliers that may negatively influence our model. To make our model cleaner, let`s
#get rid of the most important outliers, i.e. people with the salary higher than 300,000.
> df <- subset(df,income<300)
> ggplot(df,aes(card,income))+geom_boxplot()
#Looks a bit better right now, we have deleted 34 observations from 5,000 and now we can continue with the multinomial analysis.
#First of all we will pick up a reference category for each variable that is categorical, both dependant and independant.

> df$gender <- relevel(df$gender, ref = 'Male')
> df$card <- relevel(df$card, ref = "Other")
> library(nnet)
> model<- multinom(card~gender+age+income,data=df)
> summary(model)
Coefficients:
                 (Intercept) genderFemale          age     income
American Express    1.252334   -0.8324198 -0.002090151 0.01793280
Discover            1.054368   -0.4531345  0.002069272 0.01997840
Mastercard          1.141782   -0.3550897  0.005953687 0.01224755
Visa                1.281010   -0.2452058  0.004374856 0.01042260

#We now have coefficients of the model, though it is better to interpret their antilogariphms.
> summ <- summary(model)
> z <- summ$coefficients/summ$standard.errors
> pv <- pnorm(abs(z),lower.tail = F)*2
> pv
                  (Intercept) genderFemale       age       income
American Express 8.323888e-08 7.884441e-08 0.6127259 5.787055e-10
Discover         4.747736e-06 2.804839e-03 0.6065860 3.063833e-12
Mastercard       7.508746e-07 1.941748e-02 0.1351036 2.324264e-05
Visa             2.452345e-08 1.054975e-01 0.2695227 3.293312e-04
#As we can see from the model results, all categories and variables except Age are statistically significant and they differ from
#our baselines. Also we can change our baselines to see the picture from the other side.

#
> expb <- exp(coef(model))
> expb
                 (Intercept) genderFemale      age   income
American Express    3.498497    0.4349954 0.997912 1.018095
Discover            2.870162    0.6356326 1.002071 1.020179
Mastercard          3.132345    0.7011106 1.005971 1.012323
Visa                3.600272    0.7825435 1.004384 1.010477
#Judjing from this table we can see  that most commonly the person would prefere on of AE-Visa cards comparing to other. The same is with
#the income. THe richer person is the more chances for him to choose one of the most popular financial companies from the list
#comparing to our baseline category Other. Also, it is more common that females will coose Other category expecially females would not
#choose American Express and Discover whilst these two financial companies are usually chosen by people with the higher income.

> ci <-confint(model,level=0.95)    #creating confidential intervals for each category(only one is displayed) 
> ci
, , American Express

                   2.5 %       97.5 %
(Intercept)   0.79439595  1.710271122
genderFemale -1.13625360 -0.528585926
age          -0.01018326  0.006002958
income        0.01226028  0.023605325
> exci <- exp(ci)               #ci for antilogs
> pred <- fitted(model)
> pred
            Other American Express  Discover Mastercard       Visa
1    0.0669294794        0.1703180 0.2364151 0.24203430 0.28430314
2    0.0544911434        0.2382643 0.2208771 0.23381131 0.25255615

#Giving you these two rows just as refence and a proof that our model performed really poorly. The probablilities for each category
#varies around +-25%  and obviously a small probability that person will choose Other category. We may reach almost the same effect by 
#random assigning of category. The model performed poorly, though we did all of this just for the studying purposes.

#Performimg goodness of fit 
> library(nnet)
> model<- multinom(card~gender+age+income,data=df)
> model0<- multinom(card~1,data = df)
> LL1 <- logLik(model)
> LL0 <- logLik(model0)
> mccfadden <- 1 - (LL1/LL0)
> mccfadden
'log Lik.' 0.0226665 (df=16)
> n <- nrow(df)
> nagel <- (1 -exp((2/n)*(LL0-LL1)))/(1 - exp(LL0)^(2/n))
> coxsnell <- (1 -exp((2/n)*(LL0-LL1)))
> nagel
'log Lik.' 0.06569196 (df=4)
> coxsnell
'log Lik.' 0.06569196 (df=4)
# 6% of goodness of fit proves my statement above regarding the poor accuracy of the model. We have manually computed 3 different
#types of r using loglikelyhoods of fitted and null model. So we compared how much our model is better than the model without any
#dependant variables. 

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
#Ordinal Regression
#So here we continue our work with the same dataframe though now our dependent variable is jobsat (job satisfaction) and the 
#independent variables are age, gender, income and ed (education level).
> head(df)
                 jobsat age gender income ed
1   Highly dissatisfied  20 Female     31 15
2   Highly dissatisfied  22   Male     15 17
3    Somewhat satisfied  67 Female     35 14
4 Somewhat dissatisfied  23   Male     20 16

#So we will determine how accurate our four independant variables define our josat.Firstly we will do some EDA and we will perform 
#checking of assumptions. 

> table(df$jobsat)
  Highly dissatisfied      Highly satisfied Neutral Somewhat dissatisfied   Somewhat satisfied 
                967                   886          1092                  1041   		1014


> df$gender <- relevel(factor(df$gender), ref = 'Male')
> df$card <- relevel(factor(df$card), ref = 'Visa')
> model <- polr(factor(card)~gender+age+income, data = df, Hess = T)
> summary(model)
Call:
polr(formula = factor(card) ~ gender + age + income, data = df, 
    Hess = T)

Coefficients:
                 Value Std. Error t value
genderFemale -0.041519  0.0506216 -0.8202
age           0.001668  0.0014796  1.1273
income        0.001934  0.0005744  3.3667

Intercepts:
                          Value    Std. Error t value 
Visa|Other                 -0.9360   0.0804   -11.6434
Other|American Express     -0.7159   0.0798    -8.9732
American Express|Discover   0.1361   0.0793     1.7158
Discover|Mastercard         1.3131   0.0818    16.0468

Residual Deviance: 14874.12 
AIC: 14888.12 

#So now we need to interpret a model ad define which variables are statistically impotant. For this we will use Anova function from
#pakage 'car'. As we can see below there is only one variable that should be considered as the main influencer on type of card.
> Anova(model)
Analysis of Deviance Table (Type II tests)

Response: factor(card)
       LR Chisq Df Pr(>Chisq)    
gender   0.6727  1  0.4120956    
age      1.2716  1  0.2594574    
income  11.3632  1  0.0007491 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#We can do this in a different way to get detailed p-values for each factor: 
> df.coef <- coef(summary(model))
> p <- pnorm(abs(df.coef[,'t value']), lower.tail = F)*2
> p
             genderFemale                       age                    income 
             4.121111e-01              2.596005e-01              7.607381e-04 
               Visa|Other    Other|American Express American Express|Discover 
             2.479347e-31              2.881087e-19              8.619020e-02 
      Discover|Mastercard 
             6.021519e-58 
> table.coef <- cbind(df.coef, 'p value'=p)
> table.coef
                                 Value   Std. Error     t value      p value
genderFemale              -0.041519028 0.0506215918  -0.8201842 4.121111e-01
age                        0.001667982 0.0014795785   1.1273359 2.596005e-01
income                     0.001933925 0.0005744278   3.3666985 7.607381e-04
Visa|Other                -0.936042517 0.0803925502 -11.6433987 2.479347e-31
Other|American Express    -0.715910683 0.0797835012  -8.9731670 2.881087e-19
American Express|Discover  0.136059672 0.0792959615   1.7158462 8.619020e-02
Discover|Mastercard        1.313094147 0.0818291756  16.0467723 6.021519e-58

> ci <- confint(model)
> ci
                     2.5 %      97.5 %
genderFemale -0.1407471338 0.057695074
age          -0.0012312261 0.004568050
income        0.0008096361 0.003059545
#Usually if the CI does not contain 0 it means that our variable is important and does contribute to the defining of dependant variable.
#In our case gender and age are not statistically significant, though Income is crusial.

#Now we can continue to calculation our odds ratio, as these coeficient are easier to interpret. 
#
> odds.ratio <- exp(coef(model))
> odds.ratio
genderFemale          age       income 
   0.9593311    1.0016694    1.0019358 
> ci.odds <- exp(confint(model))
> ci.odds
                 2.5 %   97.5 %
genderFemale 0.8687090 1.059392
age          0.9987695 1.004578
income       1.0008100 1.003064
#So right now we can interpret our odds. We will be doing this only for the income as this is the only variable that has p-value <5%.
# The higher is income the higher chance for our observations to choose any kind of card except Visa are multiplied by 1.002. In fact
# we can see that the difference is not so crucial, but we remember that variance of income:
> summary(df$income)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   9.00   24.00   38.00   52.45   66.00  296.00 
#So increase of 0.2% for each thousand eared by respondents is somehow important for us. 

#To calculate goodness of fit we will come up with three coeficients called pseudoR
> model0 <- polr(factor(card)~1,data = df, method = 'logistic', Hess = T)
> LL0 <- logLik(model0)
> ll1 <- logLik(model)
> LL1 <- logLik(model)
> mcfadden <- 1 - LL1/LLO
Error: object 'LLO' not found
> mcfadden <- 1 - LL1/LL0
> mcfadden <- 1 - (LL1/LL0)
> n <- nrow(df)
> coxsnell <- 1 - exp((2/n)*(LL0-LL1))
> nagel <- (1 - exp((2/n)*(LL0-LL1)))/(1-exp(LL0)^(2/n))
> pseudo.r <- cbind('McFadden'=mcfadden, 'Cox-Snell'=coxsnell, 'Nagelkerke' = nagel)
> pseudo.r
        McFadden   Cox-Snell  Nagelkerke
[1,] 0.001051058 0.003146469 0.003146469
